# バッチ設計（cron）

## 目的・範囲

- フェーズ5.3「データ投入」の定期実行を、cron で運用できる形にする
- はてなブックマーク公開RSSフィードからエントリーを取得し、DBへ投入する
- 既存エントリーのブックマーク件数を、優先度別に定期更新する

## 前提

- アプリ設定は環境変数（`.env`）で注入する
- RSS/ブックマーク件数API/YahooキーフレーズAPIは `internal/infra/external/*` を利用する（バッチ側から呼び出す）
- favicon はDBに保存しない（必要時にFaviconプロキシ/APIで取得）

## 起動方式（cron）

- cron からバイナリを起動する（プロセス常駐はしない）
- 重複起動を防ぐため、ジョブごとに排他制御を行う（例: `flock` / PIDファイル等）
- 失敗時は終了コードを返し、ログに要因を残す（cron からのリトライは必要に応じて別途設計）

## ジョブ一覧

### 1) フィード投入ジョブ（予定: `cmd/fetcher`）

- 目的: RSSフィードから新規エントリーを取得し、DBへ投入する
- スケジュール: 15分ごと（フェーズ5.3の想定）
- 入力:
  - `HATENA_RSS_FEED_URLS`（`|`区切り）
  - `HATENA_API_TIMEOUT`
  - `YAHOO_APP_ID`（タグ抽出を有効化する場合）
- 出力:
  - `entries`（新規INSERT、重複はスキップ）
  - `tags` / `entry_tags`（タグ抽出を行う場合）

#### 処理フロー（概要）

1. フィードURL群を順に取得し、RSSをパースする
2. 各アイテムをEntryとして正規化（URL、タイトル、抜粋、subject、posted_at、bookmark_count）
3. 既存判定（URLユニーク制約）により重複を除外しつつ投入する
4. （任意）タイトル+抜粋からYahooキーフレーズ抽出し、上位3〜5件をタグ化して紐付ける

#### 冪等性

- エントリーはURLユニーク制約により重複投入を防ぐ
- 実装では、重複時にエラーとして落ちないようにする（例: `ON CONFLICT DO NOTHING` もしくはアプリ側で重複を検知してスキップ）

### 2) ブックマーク件数更新ジョブ（予定: `cmd/updater`）

- 目的: 既存エントリーのブックマーク件数を一括APIで効率更新する
- スケジュール: 20分ごと
- 入力:
  - `HATENA_MAX_URLS`（一括API 1リクエストあたり最大URL数）
  - `HATENA_API_TIMEOUT`
- 出力:
  - `entries.bookmark_count` の更新
  - `entries.updated_at` の更新（循環更新の基準）

#### 対象抽出（概要）

- 投稿から7日以内: 高頻度で更新
- 投稿から30日以内: 中頻度で更新
- 投稿から365日以内: 低頻度で更新
- posted-older: posted_at を見ずに更新遅れを循環更新

#### HTTP→HTTPS URL正規化

- 目的: `http:` で始まるエントリーのURLを、ブックマーク件数が多い方（http or https）に正規化する
- 既存の4パターン更新の後に実行する

##### 処理フロー

1. `url` が `http:` で始まるエントリーを `updated_at ASC` 基準で `LIMIT 25` 抽出する
2. 各エントリーについて http URL と https URL の2つを用意し、計50件を一括APIでリクエストする
3. http と https の結果を比較し、ブックマーク件数が多い方の URL・件数でエントリーを更新する（`url`, `bookmark_count`, `updated_at`）
4. https URL のエントリーが既にDBに存在する場合は、httpsエントリーを残し件数を更新した上で、httpエントリーを削除する（関連する `entry_tags`, `click_metrics` は CASCADE で自動削除）

#### 実装上の要点

- 1回の実行で4パターンを順に実行し、各パターンで `entries.updated_at ASC` を基準に `LIMIT 50` で抽出する
- 4パターン更新の後、HTTP→HTTPS URL正規化を実行する
- 一括APIはURLをチャンクに分割して呼び出す（最大50URL/リクエスト）
- 失敗したチャンクはログに残し、ジョブとしては失敗終了（再実行で回復できる前提）

### 3) アーカイブ日別件数のフル再集計ジョブ（予定: `cmd/admin archive rebuild`）

- 目的: 日別エントリー数の事前集計を全期間で再作成する
- スケジュール: 1日1回（深夜帯推奨）
- 入力:
  - DB接続情報（環境変数）
- 出力:
  - `archive_counts` の全件再生成

#### 処理フロー（概要）

1. `archive_counts` を全削除する
2. `entries` から `day` / `threshold`（5, 10, 50, 100, 500, 1000）単位で集計し再投入する

## ログ・監視

- ログ: `internal/platform/logger` 相当の構造化ログを利用し、ジョブ名・対象件数・所要時間・失敗理由を出す
- 監視: cron の実行結果（終了コード）とログ集約で検知する
- 将来: Prometheus Pushgateway 等が必要なら別途検討（現時点では必須にしない）

## 失敗時の扱い

- 外部API:
  - タイムアウトは設定値に従う
  - 取得失敗はジョブ失敗として終了し、ログに残す
- DB:
  - 接続失敗・更新失敗はジョブ失敗として終了し、ログに残す
- 再実行:
  - フィード投入は冪等であること（重複スキップ）を前提に手動/自動再実行できるようにする
  - 更新ジョブは `updated_at` の循環で次回以降に追いつく前提とする

## 運用メモ

- バッチ用バイナリは `cmd/app` と分離し、責務を明確にする（`cmd/fetcher`, `cmd/updater`）
- デプロイ先では `.env` を読み込めるようにし、cron 実行ユーザーの権限/パス/作業ディレクトリを固定する
